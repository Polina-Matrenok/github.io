# -*- coding: utf-8 -*-
"""bert_model_(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bowB5_xeCAMo0piyxvETpuekM4_pJb_i
"""

!nvidia-smi

# Since Google Colab removed the ability to connect to Google Drive from
# a non-current account, there is some hack that still allows you to do this.
#
# You need to follow the link that will appear in the output and login to the
# account that owns the desired Google Drive.
#
# After that, you need to run the next cell.
#
# https://stackoverflow.com/questions/69819761/i-cant-mount-my-another-drive-in-google-colab/70797774#70797774

!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!sudo apt-get update -qq 2>&1 > /dev/null
!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null
!google-drive-ocamlfuse

# Commented out IPython magic to ensure Python compatibility.
!sudo apt-get install -qq w3m # to act as web browser
!xdg-settings set default-web-browser w3m.desktop # to set default browser
# %cd /content
!mkdir drive
# %cd drive
!mkdir MyDrive
# %cd ..
# %cd ..
!google-drive-ocamlfuse /content/drive/MyDrive
# %cd /content

import numpy as np
import os
import pandas as pd
from tqdm.notebook import tqdm
from sklearn.model_selection import train_test_split

"""## Preprocessing"""

# PATH = '/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/data/'
PATH = '/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/data_test/'
files = next(os.walk(PATH))[2]

data_df = pd.DataFrame()
for file in tqdm(files):
    df = pd.read_csv(PATH + file)
    data_df = data_df.append(df, ignore_index=True)
data_df = data_df.drop(columns=['Unnamed: 0'])

data_df.fillna(value='_', inplace=True)

data_df.head()

data_df.X = data_df.X.apply(lambda x: ' '.join(x.split()[-35:-3]) + ' <mask> ' + ' '.join(x.split()[-3:]))

data_df.head()

data_df.shape

data_df = data_df.sample(frac=1).reset_index(drop=True)

# test_df = data_df

train_val_df, test_df = train_test_split(data_df, test_size = 0.2)
train_df, val_df = train_test_split(train_val_df, test_size = 0.2)
train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print(train_df.shape, val_df.shape, test_df.shape)

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
label_encoder.fit(train_df.y.values)
label_encoder.classes_

import matplotlib.pyplot as plt
plt.hist(label_encoder.transform(data_df.y.values))
plt.show()

train_df.shape

"""## Model"""

!pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/ruRoberta-large")

model = AutoModelForMaskedLM.from_pretrained("sberbank-ai/ruRoberta-large")

model.lm_head.decoder = torch.nn.Linear(in_features=1024, out_features=label_encoder.classes_.shape[0], bias=True)

for param in model.roberta.parameters():
    param.requires_grad = False

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"The model has {count_parameters(model):,} trainable parameters")

plt.hist(data_df.X.apply(lambda x: len(x.split())).values)
plt.show()

# from torch.utils.data import Dataset

# class MyDataset(Dataset):
#     def __init__(self, data_df, label_encoder, tokenizer):
#         super().__init__()
#         tokenizer_output = tokenizer.batch_encode_plus(list(data_df.X.values), padding=True, return_tensors='pt')
#         self.input_ids, self.attention_mask = tokenizer_output.input_ids, tokenizer_output.attention_mask
#         self.y = label_encoder.transform(data_df.y.values)
#         self.label_encoder = label_encoder
#         self.tokenizer = tokenizer

#     def __len__(self):
#         return self.y.shape[0]

#     def __getitem__(self, idx):
#         return self.input_ids[idx], self.attention_mask[idx], self.y[idx]

# train_dataset = MyDataset(train_df, label_encoder, tokenizer)
# val_dataset = MyDataset(val_df, label_encoder, tokenizer)
# test_dataset = MyDataset(test_df, label_encoder, tokenizer)

def collate_fn(batch):
    batch = np.array(batch)
    X = list(batch[:,0])
    y = list(batch[:,1])
    tokenizer_output = tokenizer.batch_encode_plus(X, padding=True, return_tensors='pt')
    return tokenizer_output.input_ids, tokenizer_output.attention_mask, torch.tensor(label_encoder.transform(y))

from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_df.values, batch_size=600, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(val_df.values[:20000], batch_size=600, shuffle=False, collate_fn=collate_fn)
test_dataloader = DataLoader(test_df.values[:20000], batch_size=1000, shuffle=False, collate_fn=collate_fn)

print(train_df.shape, val_df.shape, test_df.shape)

for X, attn_mask, y in train_dataloader:
    print(X.shape, attn_mask.shape, y.shape)
    break

optimizer = torch.optim.AdamW(model.parameters(), amsgrad=True)
criterion = torch.nn.CrossEntropyLoss()
loss_history, train_loss_history, val_loss_history, val_accuracy_history = [], [], [], []
best_val_loss = 1.0e100

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model.to(device);

import os
exp_name = 'run_2'
if not os.path.exists("/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/model_checkpoints/" + exp_name):
    os.makedirs("/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/model_checkpoints/" + exp_name, exist_ok=True)

import matplotlib.pyplot as plt
from IPython.display import clear_output
from torch.nn.utils import clip_grad_norm_


n_epochs = 5
clip = 1
n_iter_accumulation = 1
for epoch in range(n_epochs):
    model.train()
    train_loss = 0
    optimizer.zero_grad()
    for i, (X, attn_mask, y) in enumerate(train_dataloader):
        X, attn_mask, y = X.to(device), attn_mask.to(device), y.to(device)
        logits = model(input_ids=X, attention_mask=attn_mask).logits

        # print(output.logits.shape, y.shape)
        mask_token_index = (X == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]
        loss = criterion(logits[torch.arange(X.shape[0]), mask_token_index], y) / n_iter_accumulation
        loss.backward()
        clip_grad_norm_(model.parameters(), clip)
        if (i + 1) % n_iter_accumulation == 0:
            optimizer.step()
            optimizer.zero_grad()

        train_loss += loss.item()
        loss_history.append(loss.item())

        if len(loss_history) % 1 == 0:
            clear_output(wait=True)

            plt.figure(figsize=(15, 5))

            plt.subplot(131)
            plt.plot(loss_history)
            plt.xlabel("step")

            plt.subplot(132)
            plt.plot(train_loss_history, label="train loss")
            plt.plot(val_loss_history, label="val loss")
            plt.xlabel("epoch")
            plt.legend()

            plt.subplot(133)
            plt.plot(val_accuracy_history, label="val accuracy")
            plt.xlabel("epoch")
            plt.legend()

            plt.show()

    train_loss /= len(train_dataloader)
    train_loss_history.append(train_loss)

    model.eval()
    val_loss = 0
    val_accuracy = 0
    with torch.no_grad():
        for X, attn_mask, y in val_dataloader:
            X, attn_mask, y = X.to(device), attn_mask.to(device), y.to(device)
            logits = model(input_ids=X, attention_mask=attn_mask).logits

            mask_token_index = (X == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]
            loss = criterion(logits[torch.arange(X.shape[0]), mask_token_index], y)

            preds = logits[torch.arange(X.shape[0]), mask_token_index].argmax(-1)
            accuracy = (preds == y).float().mean()

            val_loss += loss.item()
            val_accuracy += accuracy.item()

    val_loss /= len(val_dataloader)
    val_accuracy /= len(val_dataloader)
    val_loss_history.append(val_loss)
    val_accuracy_history.append(val_accuracy)

    if best_val_loss > val_loss:
        torch.save(
            {
                'model': model,
                'model_weights': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch
            },
            f'/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/model_checkpoints/{exp_name}/model_best.pth'
        )
        best_val_loss = val_loss

torch.save(
    {
        'model': model,
        'model_weights': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'epoch': epoch
    },
    f'/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/model_checkpoints/{exp_name}/model_last_iter.pth'
    )

torch.save(
    {
        'model': model,
        'model_weights': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'epoch': epoch
    },
    f'/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/model_checkpoints/{exp_name}/model_last_iter.pth'
    )

model.load_state_dict(torch.load(f'/content/drive/MyDrive/Deep_Learning/Diploma_Fedor/model_checkpoints/{exp_name}/model_last_iter.pth')['model_weights'])

from tqdm.notebook import tqdm

test_accuracy = 0
with torch.no_grad():
    for X, attn_mask, y in tqdm(test_dataloader):
        X, attn_mask, y = X.to(device), attn_mask.to(device), y.to(device)
        logits = model(input_ids=X, attention_mask=attn_mask).logits

        mask_token_index = (X == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]
        # loss = criterion(logits[torch.arange(X.shape[0]), mask_token_index], y)

        preds = logits[torch.arange(X.shape[0]), mask_token_index].argmax(-1)
        accuracy = (preds == y).float().mean()

        # test_loss += loss.item()
        test_accuracy += accuracy.item()
test_accuracy /= len(test_dataloader)

#print(f'test_accuracy: {test_accuracy}')

print(f'test_accuracy: {test_accuracy}')